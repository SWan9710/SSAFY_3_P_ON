{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T00:57:07.192742600Z",
     "start_time": "2023-10-31T00:57:05.597901500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (1.25.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from konlpy) (4.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T00:57:08.721528600Z",
     "start_time": "2023-10-31T00:57:07.194743600Z"
    }
   },
   "id": "c65a4432d5160585"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.14.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.25.1)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:21.077915900Z",
     "start_time": "2023-10-31T01:01:18.994127Z"
    }
   },
   "id": "7d008817e088e8a1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:23.020096800Z",
     "start_time": "2023-10-31T01:01:22.993167700Z"
    }
   },
   "id": "5ab9d3b19d2e3b33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 토크나이즈"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f789de2c7405b41"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, servicedic=None): # userdic 인자에는 사용자 정의 사전 파일 경로 입력가능\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=servicedic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            # 주격조사, 보격조사, 관형격조사, 목적격조사, 부사격조사, 호격조사, 인용격조사\n",
    "            'JX', 'JC',\n",
    "            # 보조사, 접속조사\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            # 마침표,물음표,느낌표(SF), 쉼표,가운뎃점,콜론,빗금(SP), 따옴표,괄호표,줄표(SS), 줄임표(SE), 붙임표(물결,숨김,빠짐)(SO)\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            # 선어말어미, 종결어미, 연결어미, 명사형전성어미, 관형형전성어미\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "            # 명사파생접미사, 동사파생접미사, 형용사파생접미사\n",
    "        ]\n",
    "\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:25.236345300Z",
     "start_time": "2023-10-31T01:01:25.209952500Z"
    }
   },
   "id": "121775e1676e0df2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce53fbd483b140a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('구희영', 'NNP'), ('보', 'VV'), ('싶', 'VX')]\n",
      "['구희영', '보', '싶']\n"
     ]
    }
   ],
   "source": [
    "sent = \"구희영 보고싶어\"\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(servicedic='./files/service_dic.tsv')\n",
    "\n",
    "# 형태소 분석기 실행\n",
    "pos = p.pos(sent)\n",
    "\n",
    "# 품사 태그와 같이 키워드 출력\n",
    "ret = p.get_keywords(pos, without_tag=False)\n",
    "print(ret)\n",
    "\n",
    "# 품사 태그 없이 키워드 출력\n",
    "ret = p.get_keywords(pos, without_tag=True)\n",
    "print(ret)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:32.196586700Z",
     "start_time": "2023-10-31T01:01:31.017126100Z"
    }
   },
   "id": "6d9391c62d3c04ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 단어 사전"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6aa3d7782df17a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mdict\u001B[39m \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m corpus_data:\n\u001B[1;32m---> 30\u001B[0m     pos \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m pos:\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28mdict\u001B[39m\u001B[38;5;241m.\u001B[39mappend(k[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn[11], line 28\u001B[0m, in \u001B[0;36mPreprocess.pos\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpos\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence):\n\u001B[1;32m---> 28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkomoran\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\konlpy\\tag\\_komoran.py:92\u001B[0m, in \u001B[0;36mKomoran.pos\u001B[1;34m(self, phrase, flatten, join)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence:\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjki\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgetTokenList()\n\u001B[0;32m     93\u001B[0m result \u001B[38;5;241m=\u001B[39m [(token\u001B[38;5;241m.\u001B[39mgetMorph(), token\u001B[38;5;241m.\u001B[39mgetPos()) \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m result]\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m join:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 단어 사전 파일 생성 코드입니다.\n",
    "# 챗봇에 사용하는 사전 파일\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 말뭉치 데이터 읽어오기\n",
    "movie_review = pd.read_csv('data/영화리뷰.csv')\n",
    "purpose = pd.read_csv('data/용도별목적대화데이터.csv')\n",
    "topic = pd.read_csv('data/주제별일상대화데이터.csv')\n",
    "common_sense = pd.read_csv('data/일반상식.csv')\n",
    "\n",
    "movie_review.dropna(inplace=True)\n",
    "purpose.dropna(inplace=True)\n",
    "topic.dropna(inplace=True)\n",
    "common_sense.dropna(inplace=True)\n",
    "\n",
    "text1 = list(movie_review['document'])\n",
    "text2 = list(purpose['text'])\n",
    "text3 = list(topic['text'])\n",
    "text4 = list(common_sense['query']) + list(common_sense['answer'])\n",
    "\n",
    "corpus_data = text1 + text2 + text3 + text4\n",
    "\n",
    "# 말뭉치 데이터에서 키워드만 추출해서 사전 리스트 생성\n",
    "p = Preprocess()\n",
    "dict = []\n",
    "for c in corpus_data:\n",
    "    pos = p.pos(c)\n",
    "    for k in pos:\n",
    "        dict.append(k[0])\n",
    "\n",
    "# 사전에 사용될 word2index 생성\n",
    "# 사전의 첫 번째 인덱스에는 OOV 사용\n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV', num_words=100000)\n",
    "tokenizer.fit_on_texts(dict)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "# 사전 파일 생성\n",
    "f = open(\"chatbot_dict.bin\", \"wb\")\n",
    "try:\n",
    "    pickle.dump(word_index, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:39.082180Z",
     "start_time": "2023-10-31T01:01:32.191063700Z"
    }
   },
   "id": "603e4b574520d63a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST ) 단어 사전"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64cd02e4d37920"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구희영 1\n",
      "언제 162\n",
      "만나 359\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 불러오기\n",
    "f = open(\"chatbot_dict.bin\", \"rb\")\n",
    "word_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sent = \"구희영 언제 만나더라?\"\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(servicedic='files/service_dic.tsv')\n",
    "\n",
    "# 형태소 분석기 실행\n",
    "pos = p.pos(sent)\n",
    "\n",
    "# 품사 태그 없이 키워드 출력\n",
    "keywords = p.get_keywords(pos, without_tag=True)\n",
    "for word in keywords:\n",
    "    try:\n",
    "        print(word, word_index[word])\n",
    "    except KeyError:\n",
    "        # 해당 단어가 사전에 없는 경우 OOV 처리\n",
    "        print(word, word_index['OOV'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:48.744223300Z",
     "start_time": "2023-10-31T01:01:46.504079400Z"
    }
   },
   "id": "5609cb2c83007ec1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 텍스트 전처리\n",
    "- 토크 나이즈\n",
    "- 생성한 단어 사전으로 단어를 정수 인코딩"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c36f69378d74daa"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='chatbot_dict.bin', servicedic=None): # userdic 인자에는 사용자 정의 사전 파일 경로 입력가능\n",
    "        \n",
    "        if (word2index_dic != ''):\n",
    "            f = open(word2index_dic, 'rb')\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "            print('word_dic 로드 성공')\n",
    "        else:\n",
    "            self.word_index = None\n",
    "            print('word_dic 로드 실패')\n",
    "        \n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=servicedic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            # 주격조사, 보격조사, 관형격조사, 목적격조사, 부사격조사, 호격조사, 인용격조사\n",
    "            'JX', 'JC',\n",
    "            # 보조사, 접속조사\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            # 마침표,물음표,느낌표(SF), 쉼표,가운뎃점,콜론,빗금(SP), 따옴표,괄호표,줄표(SS), 줄임표(SE), 붙임표(물결,숨김,빠짐)(SO)\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            # 선어말어미, 종결어미, 연결어미, 명사형전성어미, 관형형전성어미\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "            # 명사파생접미사, 동사파생접미사, 형용사파생접미사\n",
    "        ]\n",
    "\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "    \n",
    "    \n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "        \n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:01:48.796224400Z",
     "start_time": "2023-10-31T01:01:48.742224300Z"
    }
   },
   "id": "48b37fe5392a1aa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST ) 텍스트 전처리"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce2b1c64a0b15be"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dic 로드 성공\n",
      "[('구희영', 'NNP'), ('언제', 'MAG'), ('만나', 'VV')]\n",
      "['구희영', '언제', '만나']\n",
      "[422, 3885, 1042, 1, 3817, 287, 1, 56, 13]\n"
     ]
    }
   ],
   "source": [
    "sent = \"구희영 언제 만나\"\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(servicedic='./files/service_dic.tsv')\n",
    "\n",
    "# 형태소 분석기 실행\n",
    "pos = p.pos(sent)\n",
    "\n",
    "# 품사 태그와 같이 키워드 출력\n",
    "ret = p.get_keywords(pos, without_tag=False)\n",
    "print(ret)\n",
    "\n",
    "# 품사 태그 없이 키워드 출력\n",
    "ret = p.get_keywords(pos, without_tag=True)\n",
    "print(ret)\n",
    "\n",
    "# 단어 임베딩 리스트 출력\n",
    "embedded_lst = p.get_wordidx_sequence(keywords=sent)\n",
    "print(embedded_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:03:52.950724700Z",
     "start_time": "2023-10-31T01:03:50.945858300Z"
    }
   },
   "id": "8f8527c36a7660be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 의도 분류 모델\n",
    "- import\n",
    "- 데이터 불러오기\n",
    "- 전처리\n",
    "- 데이터 변환 (텐서화, 학습/validation/test 데이터 나눔\n",
    "- 하이퍼 파라미터 설정\n",
    "- CNN 모델 정의\n",
    "- CNN 모델 생성\n",
    "- train\n",
    "- validation\n",
    "- test\n",
    "- 모델 저장"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4922f8b0c4cafbc1"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:07:46.794362700Z",
     "start_time": "2023-10-31T01:07:46.751862200Z"
    }
   },
   "id": "f5155a6a3fa904c0"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train_data.csv\")\n",
    "text = data['text'].tolist()\n",
    "label = data['label'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:14:38.478383900Z",
     "start_time": "2023-10-31T01:14:38.422383500Z"
    }
   },
   "id": "6ffaca3bac9a4208"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dic 로드 성공\n"
     ]
    }
   ],
   "source": [
    "p = Preprocess(word2index_dic='chatbot_dict.bin',\n",
    "               servicedic='files/service_dic.tsv')\n",
    "\n",
    "# Data preprocess\n",
    "sequences = []\n",
    "for sentence in text:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:14:55.378368300Z",
     "start_time": "2023-10-31T01:14:43.805878700Z"
    }
   },
   "id": "7b010f261fcbae12"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# set padding length & pad to sequences\n",
    "from config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# data to tensor\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, label))\n",
    "ds = ds.shuffle(len(text))\n",
    "\n",
    "# set train & validation & test size\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(100)\n",
    "val_ds = ds.take(train_size).take(val_size).batch(100)\n",
    "test_ds = ds.take(train_size + val_size).take(test_size).batch(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:18:16.971043600Z",
     "start_time": "2023-10-31T01:18:16.879045Z"
    }
   },
   "id": "4fe25b7600f6b6d8"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 3\n",
    "VOCAB_SIZE = len(p.word_index) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:18:17.095039400Z",
     "start_time": "2023-10-31T01:18:17.082040400Z"
    }
   },
   "id": "43d45f49a887ce18"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# CNN model definition\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "\n",
    "logits = Dense(4, name='logits')(dropout_hidden)\n",
    "predictions = Dense(4, activation=tf.nn.softmax)(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:27:33.971178900Z",
     "start_time": "2023-10-31T01:27:33.764182Z"
    }
   },
   "id": "f112c329207ab418"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# CNN model create\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:27:35.339576500Z",
     "start_time": "2023-10-31T01:27:35.286577500Z"
    }
   },
   "id": "cb295aa99ea7c8ca"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "170/170 [==============================] - 37s 210ms/step - loss: 0.3709 - accuracy: 0.8523 - val_loss: 0.0287 - val_accuracy: 0.9918\n",
      "Epoch 2/3\n",
      "170/170 [==============================] - 34s 202ms/step - loss: 0.0324 - accuracy: 0.9909 - val_loss: 0.0085 - val_accuracy: 0.9975\n",
      "Epoch 3/3\n",
      "170/170 [==============================] - 36s 213ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.0101 - val_accuracy: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x219015715b0>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:29:24.585104100Z",
     "start_time": "2023-10-31T01:27:36.689499Z"
    }
   },
   "id": "697b2e5e86d203"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 19ms/step - loss: 0.0042 - accuracy: 0.9979\n",
      "Accuracy: 99.793816\n",
      "loss : 0.004231\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(\"Accuracy: %f\" % (accuracy * 100))\n",
    "print(\"loss : %f\" % (loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:30:00.881153700Z",
     "start_time": "2023-10-31T01:30:00.349195500Z"
    }
   },
   "id": "5289a6af8646bae7"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('intent_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:30:02.836844900Z",
     "start_time": "2023-10-31T01:30:01.946990500Z"
    }
   },
   "id": "3a2524f1698bf75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST ) 의도 분류 모델"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533247b2590e6b28"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dic 로드 성공\n"
     ]
    }
   ],
   "source": [
    "p = Preprocess(word2index_dic='chatbot_dict.bin',\n",
    "               servicedic='files/service_dic.tsv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:34:28.006767Z",
     "start_time": "2023-10-31T01:34:26.497223500Z"
    }
   },
   "id": "2280802a6eb729b1"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "import gc\n",
    "\n",
    "# 의도 분류 모델 모듈\n",
    "class IntentModel:\n",
    "    def __init__(self, model_name, preprocess):\n",
    "\n",
    "        # 의도 클래스별 레이블블\n",
    "        self.labels = {0: \"장소\", 1: \"시간\", 2: \"사람\", 3 : \"내용\"}\n",
    "\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 텍스트 전처리기\n",
    "        self.p = preprocess\n",
    "\n",
    "    # 의도 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 단어 시퀀스 벡터 크기\n",
    "        from config.GlobalParams import MAX_SEQ_LEN\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "        return predict_class.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:39:23.378520Z",
     "start_time": "2023-10-31T01:39:23.350678700Z"
    }
   },
   "id": "6689381a5fb216cb"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "intent = IntentModel(model_name='intent_model.h5', preprocess=p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:39:26.690858100Z",
     "start_time": "2023-10-31T01:39:25.246073300Z"
    }
   },
   "id": "e15ba585f376e65d"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "==============================\n",
      "토요일 약속 장소 어디지?\n",
      "의도 예측 클래스 :  0\n",
      "의도 예측 레이블 :  장소\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "==============================\n",
      "언제 구희영 만나더라?\n",
      "의도 예측 클래스 :  1\n",
      "의도 예측 레이블 :  시간\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "==============================\n",
      "13일 약속 누구누구 가\n",
      "의도 예측 클래스 :  2\n",
      "의도 예측 레이블 :  사람\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "==============================\n",
      "부산대 누구랑 가더라?\n",
      "의도 예측 클래스 :  2\n",
      "의도 예측 레이블 :  사람\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "==============================\n",
      "구희영 보고싶어\n",
      "의도 예측 클래스 :  1\n",
      "의도 예측 레이블 :  시간\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "==============================\n",
      "디띵방 투표 기간 언제까지야?\n",
      "의도 예측 클래스 :  1\n",
      "의도 예측 레이블 :  시간\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "==============================\n",
      "일요일에 뭐하기로 했지\n",
      "의도 예측 클래스 :  3\n",
      "의도 예측 레이블 :  내용\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "query = \"토요일 약속 장소 어디지?\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "\n",
    "query = \"언제 구희영 만나더라?\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "\n",
    "query = \"13일 약속 누구누구 가\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "\n",
    "query = \"부산대 누구랑 가더라?\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "\n",
    "query = \"구희영 보고싶어\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "\n",
    "query = \"디띵방 투표 기간 언제까지야?\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n",
    "query = \"일요일에 뭐하기로 했지\"\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "print(\"=\"*30)\n",
    "print(query)\n",
    "print(\"의도 예측 클래스 : \", predict)\n",
    "print(\"의도 예측 레이블 : \", predict_label)\n",
    "print(\"=\"*30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T01:39:41.539579700Z",
     "start_time": "2023-10-31T01:39:41.065265200Z"
    }
   },
   "id": "16b167e5c3d23d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c82c2a279c4d251"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
